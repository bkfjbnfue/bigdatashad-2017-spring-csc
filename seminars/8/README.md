## Семинар 8-9.
### Работа со SparkStreaming

На данном семинаре будут рассмотрены:

- Знакомство с реализацией программ на SparkStreaming
- Запуск и тестирование программы на SparkStreaming c помощью утилиты netcat
- Работа с cli фреймворка Zookeeper
- Работа с cli фреймворка Kafka
- Работа с источником данных Kafka-topic в SparkStreaming
- Вычисления для последнего batch, вычисления “в временном окне”, генерация “состояния” в SparkStreaming.

После данного семинара студент должен уметь:

- Реализовывать программы с использованием SparkStreaming
- Запускать программы на SparkStreaming на кластере и тестировать свои программы с использованием утилиты netcat
- Подключаться к cli Zookeeper, проверять установленные блокировки
- Подключаться к cli Kafka, просматривать существующие topic, получать информацию о работающих брокерах, просматривать сообщения topic-ов с помощью базового потребителя kafka-console-consumer
- Запускать программы на SparkStreaming с использованием источника данных topic сообщений в Kafka.


#### Материалы семинара
1. Для реализации программы на SparkStreaming необходимо в программе python-скрипте  сделать import необходимых пакетов

        from pyspark import SparkContext
        from pyspark.streaming import StreamingContext
После импорта следует создать объект контекста Spark, в конструкторе которого возможно указать необходимые настройки конфигурации. Стоит отметить, что все настройки конфигурации возможно задать через специальные параметры в командной строке. После создания контекста Spark необходимо создать контекст SparkStreaming, в конструктор которого передать созданные контекст Spark, а также размер batch в секундах.

        sc = SparkContext(appName="PythonStreamingNetcat")
        ssc = StreamingContext(sc, 10)
После создания необходимых контекстов возможно перейти к созданию стрима. Тип создаваемого стрима определяется в зависимости от источника данных, поверх которого вы строите свои вычисления. В простейшем случае (в основном для тестов) возможно создать стрим поверх канала TCP-сообщений на определенном порту. Для этого необходимо запустить утилиту `netcat` (она же `nc`), с помощью которой возможно передать в tcp-“канал” сообщения, написав сообщение в командном окне. Запустить утилиту возможно следующей командой

        $ netcat -l 0.0.0.0 9999
Такая команда приводит к созданию TCP-подключения с портом 9999, для которого верно, что все сообщения в стандартном вводе передаются на сетевой вывод и наоборот. Проще говоря, все, что будет написано в stdout можно будет прочитать на порту 9999 со всех сетевых адресов машины.

    Замечание: не получится открыть в браузере страницу с данным портом и читать сообщения, потому что нет необходимых http заголовков.

    Тем не менее, данная утилита позволяет создать поверх себя стрим в SparkStreaming.

    Для создания стрима поверх tcp-канала, поднятого утилитой `netcat`, необходимо в программе SparkStreaming создать объект SocketTextStream следующей командой

        lines = ssc.socketTextStream(ip_address_str, port_int)
После выполнения данной команды будет создан DStream, для которого будут доступен программный интерфейс SparkStreaming описанный в [документации](http://spark.apache.org/docs/latest/streaming-programming-guide.html#initializing-streamingcontext).

    Например, возможно вычислить количество строк, поступающих в tcp-канал следующей трансформацией `counts = lines.count()`. Далее в коде написан другой пример трансформаций. Вывести на экран вычисленное значение возможно методом `print()`  объекта DStream.

    Таким образом, вся простейшая программа, которая разбирает приходящие строки и вычисляет количество каждого префикса до первого пробела, будет выглядеть следующим образом:

        #!/usr/bin/env python
        import sys
        from pyspark import SparkContext
        from pyspark.streaming import StreamingContext

        def parseLine(line):
            split_values = line.split(' ')
            return (split_values[0], 1)

        if __name__ == "__main__":
            sc = SparkContext(appName="PythonStreamingWC")
            ssc = StreamingContext(sc, 10)

            lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
            objects = lines.map(parseLine).reduceByKey(lambda a, b: a+b)
            print("--------------------------------------------")
            objects.pprint()
            ssc.start()
            ssc.awaitTermination()

2. Для запуска написанной программы необходимо выполнить команду `spark-submit` на кластере, указав следующие параметры (см. подробнее в [инструкции по работе с кластером](https://docs.google.com/document/d/125jtuQgFLmPRfq7yY2KUVA7VaQ9-sl04xKeM6KOTYTo/edit)):

        $ spark-submit
            --master yarn-client \
            --num-executors 2 \
            --executor-cores 1 \
            --executor-memory 2048m \
            netcat.py "hadoop2-00.yandex.ru" 9999
В случае указания в качестве мастера `yarn-client` драйвер-программа SparkStreaming запустится на машине, на которой вызвана команда `spark-submit`; в output будут переданы все сообщения, которые выводятся с помощью команды print. Программа завершит свою работу после получения сигнала sigint (Ctrl+C).

    В случае указания в качестве мастера `yarn-cluster` драйвер-программа SparkStreaming запустится в контейнере yarn и туда будет передан весь output. После выполнения команды Ctrl+C программа не завершит свою работу, а только перестанет писать log-сообщения. Завершить команду необходимо через утилиту yarn.

    Вначале необходимо узнать ip приложения с помощью команды

        $ yarn application -list
    и завершить приложение командой:

        $yarn application -kill $app_id

3. Для получения информации о блокировках на Zookeeper необходимо
подключиться к cli zookeeper, выполнив команду:

        $ zookeeper-client -server hadoop2-10:2181
В открывшемся интерфейсе можно выполнить команду `help`, чтобы узнать все доступные функции cli. Через cli возможно создать znode, просмотреть данные, соответствующие znode.

    С помощью команды `ls /brokers/ids` можно просмотреть блокировки работающих брокеров Kafka. Через команды `get /brokers/ids/556`, `get /brokers/ids/557` можно просмотреть информацию о блокировках брокеров Kafka.

4. Для получения информации о работе Kafka cli необходимо запустить команду:

        $kafka-topics --zookeeper hadoop2-10:2181
Для получения информации о списке доступных  топиков необходимо выполнить команду

        $kafka-topics --zookeeper hadoop2-10:2181 --list
Для получения информации о конкретном топике необходимо выполнить команду

        $kafka-topics --zookeeper hadoop2-10:2181 --describe bigdatashad-2016
Для просмотра содержимого топика в stdout необходимо выполнить команду

        $kafka-console-consumer --zookeeper hadoop2-10:2181 --topic bigdatashad-2016
После выполнения данной команды на экран будут выводится сообщения из указанного топика до момента, пока вы не нажмёте Ctrl+C.

5. Для обработки сообщений из топика Kafka в приложениях SparkStreaming необходимо создать стрим поверх сообщений Kafka. Для этого необходимо:

    - подключить `jar /opt/cloudera/parcels/CDH/jars/spark-streaming-kafka_2.10-1.6.0-cdh5.9.0.jar` в процесс выполнения `spark-submit` (см. ниже пример запуска)
    - сделать `from pyspark.streaming.kafka import KafkaUtils`
    - в вашем приложении SparkStreaming создать объект стрима

            kvs =
                KafkaUtils.createStream(
                    ssc, // streamingContext
                    zkQuorum, // ZK quorum
                    "Spark-streaming-consumer-id", //consumer group id
                    {topic: 2} // per-topic number of Kafka partitions to consume
                )

    Важное замечание, что все стримы с одинаковым названием consumer group id будут вычитывать одновременно топик, как следствие сообщения будут “разделяться” между приложениями SparkStreaming. Это означает, что часть сообщений придут на первое приложение, и часть на второе. Обработка потока одновременно в нескольких приложениях в составе одной consumer group обычно требуется, для анализа потока с высокой интенсивностью, то есть когда одно приложение  SparkStreaming не успевает считывать поток и offset чтения отстает от offset конца топика.

    В домашнем задании для обработки потока достаточно одного приложения SparkStreaming, поэтому вам необходимо указать уникальный идентификатор потока в своем домашнем задании.

6. Вычисления в приложении SparkStreaming возможны для последнего batch, для всех batch в временном окне, и для всех пришедших данных в стриме.

В рамках домашнего задания вам необходимо будет выполнить все три типа вычислений.


