## Семинар 3
### Приемы для расчета метрик из hw1

План семинара:

- продолжение рассказа про Hadoop streaming
- идеи для подсчета метрик из hw1

#### Еще про streaming
Давайте соберем все, что нужно (явно или нет) указать, запрограммировать, настроить  для запуска job’ы на Hadoop:

- Input path
- Input format, типы input key, value
- Mapper
- Mapper output format, типы output key, value
- Combiner
- Comparator
- Grouping comparator
- Partitioner
- Reducer
- Reducer output format, типы output key, value
- Число reducer’ов

При работе с Hadoop в режиме streaming (используя только скриптовые языки и команды) появляются некоторые ограничения:

- все форматы - текстовые
- ключи и значения - тоже, но можно указать разделитель между ключом и значением (по умолчанию это первый tab)
- Grouping comparator - это правило, по которому группируются записи с равными ключами на reduce-стадии, его определяет разработчик непосредственно в скрипте-редьюсере. В материалах Семинара 2 можно найти его в примерах редьюсеров, оно выглядит обычно как "`if key != current_key:`"
- Partitioner - это правило разделения ключей по редьюсерам. По умолчанию Hadoop отправляет одинаковые ключи на один редьюсер. Но это можно поменять - см. п.6 Семинара 2, там partitioner считает номер редьюсера только по первому полю ключа.

Comparator определяет правило сортировки записей перед после map-стадии перед отправкой их в reducer. Сравниваются клюси записей, по умолчанию сортировка лексикографическая. Можно это поменять, см. Семинар 2.

Combiner позволяет произвести свертку данных, полученных на map-стадии. Вывод каждого маппера отсортирован по правилу, определенному в comparator'e. Убедитесь в этом: возьмите любую задачу и отключите reduce-стадию (число редьюсеров = 0) - файлы `part-m-xxxxx` должны быть отсортированы.

Свойства combiner'а:
- работает сразу после map-стадии, на той же ноде
- входные записи - это выходные записи mapper’а, но уже упорядоченные при помощи comparator’а
- может быть запущен на выводе одного маппера от 0 до нескольких раз. Отсюда: типы ключа и значений менять не может
- если преобразование, осуществляемое reducer’ом, коммутативно и ассоциативно, его можно использовать в качестве combiner’а.

Например, в материалах Семинара 2 в п.2 (uniq) и п.5 (hits per user) reducer можно использовать в качестве combiner'а. А в п.6 (подсчет сессий) combiner написать вряд ли получится. Довольно часто, когда combiner можно реализовать, но он отличается от редьюсера.

Combiner уменьшает объем данных, пересылаемых на reducer, ускоряет работу задачи и снижает нагрузку на кластер. Старайтесь использовать его при каждом удобном случае.

#### Идеи для подсчета метрик

##### Total users, total hits
Cколько каждый пользователь заходил на сайт посчитать просто - это аналог wordcount. Что дальше?

**Вариант 1:** reduce по user_id, отправить все на один reducer и получить в результате 2 числа: число пользователей и хитов. Минус: один reducer, неэффективно.

**Вариант 2:** reduce по user_id, но много reducer’ов. На выходе: набор частичных сумм (с каждого редьюсера) хитов и числа пользователей. Можно ли эти величины складывать?

Ответ: можно. Хиты всегда можно, а пользователей можно из-за reduce by user_id, их множества не пересекаются.

Посчитать сумму можно и локально (на сервере, откуда запускается задача): скачиваем выходные файлы задачи, складываем и записываем обратно на кластер или в локальную директорию с результатом (откуда будет читать ваш веб-сервер). Это - допустимая локальная работа, т.к. каждый выходной файл редьюсера невелик (содержит только два числа).

**NB:** избегайте локальной работы с большими данными. Например, скачивать список всех пользователей локально недопустимо: это может загрузить сеть, занять все место на диске.

Итак, второй вариант требует больше разработки, зато лучше распараллеливается на кластере, а значит работает быстрее.

Задание: получится ли добавить в эту задачу еще и подсчет сессий. Хорошо, когда несколько метрик получается считать в рамках одной MR-задачи.

##### Top 10 pages
Статистика url - hits считается так же, как user_id - hits. Теперь нужна вторая MR-задача для выбора топ10. Какой она будет?

**Вариант 1:** сортировка (числовая, по убыванию) и reduce by hits, один редьюсер. Получаем глобально отсортированный список страниц, в частности - top10. Минус: неэффективно, избыточно.
Вопрос: как избежать полной сортировки и выбрать топ?

**Вариант 2:** использовать структуру наподобие кучи (heap) на мапперах для выбора топ10 страниц. Это допустимо, т.к.  для heap используем ограниченный объем памяти. Дальше топ10 с каждого маппера скачиваем локально, сортируем, получаем окончательный топ10. Это тоже допустимо, т.к. работаем с 10хM записями, M - число мапперов.

**NB:** не стоит использовать контейнеры в памяти на мапперах и редьсерах, не лимитируя каким-то образом занимаемую память.

Вопрос: обычно использование контейнеров в памяти (tree, hast table, array) можно заменить еще одной map-reduce стадией. Интересно узнать задачи, для которых это не так.

###### Users by country
Можно решить в два шага.

**Шаг 1**: добавляем в логи информацию о стране, т.е. преобразуем ip в код страны. Структуру для быстрого поиска по геобазе выбираете самостоятельно (например, сортированный массив). Считаем, что геобаза в память помещается. Тут важно приложить файл с геобазой к задаче, т.е. чтобы она попала в т.н. distributed cache. Зачитать геобазу в память и отпределить geo_id для каждой строки лога можно прямо на map-стадии. Такой прием называется map-side join.

**Шаг 2**: считаем уникальных пользователей в каждой стране, для этого сортируем по по паре (country_code, user_id). Дальше важно, чтобы все записи одной страны попали на один редьюсер. Т.е. ключ - пара (country_code, user_id), партиционирование - только по country_code. Получается, что используем идею secondary sort.

##### New users, lost users
Для подсчета new users надо сделать join двух множеств: пользователей за последние несколько дней и за текущий день (lost users - аналогично). Но тут уже нельзя положиться, что одно из множеств влезет в память, т.е. техника map-side join (см. выше) не подходит.

Тогда идея такая: на map-стадии будем каждую запись метить, из какого она множества, т.е. выдавать пары user_id, tag, где например tag = 0, если пользователь из предыдущих дней и 1 - если из текущего. Далее опять secondary sort: сортируем по (user_id, tag), а partitioner смотрит только на user_id. Чтобы все записи одного пользователя попали на один редьюсер. При этом, получается удобно, что сначала для пользователя будут идти все его записи в tag=0, потом - с tag=1.

Этот подход называется reduce-side join. Для того, чтобы маппер правильно пометил тегами записи, можно воспользоваться переменными окружения, которые выставляет Hadoop. Так, в `mapreduce_map_input_file` находится имя файла, который читает маппер (точнее, сплит из которого читает маппер). Так что вычислить значение тега можно в самом начале маппера, один раз для всех строк.

##### Работа с HDFS
HDFS кластера подмонтирована к файловой системе сервера в /hdfs (или ./hdfs) с помощью FUSE. Это удобно, но работает ненадежно. Поэтому лучше использовать эти директории только при разработке. А в регулярных скриптах лучше обшяться в HDFS командами `hadoop fs` или `hdfs dfs` (см. материалы Семинара 1).


##### Заключение
Соберем все идеи вместе:

- комбинирование распределенной работы и локальной
- использование heap для топ10
- map-side join
- reduce-side join
- несколько метрик в одной MapReduce-задаче
- использование переменных окружения Hadoop

